{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b92f03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee0887",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e81a5e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# IMPORTANT: Library Dependencies\n",
    "# ============================================\n",
    "\n",
    "# pip install --upgrade transformers bitsandbytes accelerate\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# ============================================\n",
    "# STORY PROMPT\n",
    "# ============================================\n",
    "\n",
    "STORY_PROMPT = \"\"\"You are a master storyteller. Your task is to rewrite the classic fairy tale 'The Shoemaker and the Elves' into a **dystopian science-fiction story**.\n",
    "\n",
    "**Your rewrite must adhere to the following rules but should also go beyond them with unexpected twists not stated below:**\n",
    "* **Setting:** A grimy, neon-lit metropolis in the year 2242, where citizens are judged by their cybernetic enhancements.\n",
    "* **The 'Shoemaker':** He is a reclusive old craftsman who illegally repairs and builds custom bio-mechanical limbs for outcasts.\n",
    "* **The 'Elves':** They are a swarm of self-replicating nanobots that are believed to be a myth.\n",
    "* **The 'Shoes':** The nanobots are not making shoes; they are performing impossibly intricate upgrades on the limbs the craftsman leaves on his workbench overnight.\n",
    "* **Tone:** The story should be grim, mysterious, and slightly hopeful.\n",
    "* **Ending:** The story must end with the craftsman discovering the nanobots and leaving a small charging plate with a micro-drop of refined energy for them as a gift, without ever seeing them directly.\"\"\"\n",
    "\n",
    "# ============================================\n",
    "# MODEL LOADING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def load_base_model(base_model_id=\"google/gemma-2-2b-it\"):\n",
    "    \"\"\"Load the original base model without fine-tuning\"\"\"\n",
    "    \n",
    "    print(\"Loading original base model with quantization...\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    base_model.eval()\n",
    "    print(\"✓ Original model loaded successfully!\")\n",
    "    return base_model, tokenizer\n",
    "\n",
    "# ============================================\n",
    "# GENERATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def generate_story(model, tokenizer, prompt, strategy=\"greedy\", temperature=1.0, \n",
    "                   top_k=50, top_p=0.9, num_beams=1, max_length=1024):\n",
    "    \"\"\"Generate story with different decoding strategies\"\"\"\n",
    "    \n",
    "    # Format input in Gemma's chat template\n",
    "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generation parameters based on strategy\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": max_length,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    if strategy == \"greedy\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": False,\n",
    "            \"num_beams\": 1,\n",
    "        })\n",
    "        \n",
    "    elif strategy == \"sampling\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "        })\n",
    "        \n",
    "    elif strategy == \"top_k\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": top_k,\n",
    "        })\n",
    "        \n",
    "    elif strategy == \"top_p\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "        })\n",
    "        \n",
    "    elif strategy == \"beam\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": False,\n",
    "            \"num_beams\": num_beams,\n",
    "            \"early_stopping\": True,\n",
    "        })\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the model's response\n",
    "    if \"<start_of_turn>model\" in generated_text:\n",
    "        response = generated_text.split(\"<start_of_turn>model\")[-1].strip()\n",
    "    else:\n",
    "        response = generated_text\n",
    "    \n",
    "    return response\n",
    "\n",
    "# ============================================\n",
    "# FILE SAVING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def save_story_to_file(filepath, story_text, prompt, strategy, temperature, model_type, top_k=None, top_p=None):\n",
    "    \"\"\"Save generated story to a text file with metadata\"\"\"\n",
    "    \n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        # f.write(\"=\"*80 + \"\\n\")\n",
    "        # f.write(f\"GENERATED STORY - {model_type.upper()}\\n\")\n",
    "        # f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        # f.write(f\"Model Type: {model_type}\\n\")\n",
    "        # f.write(f\"Decoding Strategy: {strategy}\\n\")\n",
    "        # if temperature is not None:\n",
    "        #     f.write(f\"Temperature: {temperature}\\n\")\n",
    "        # if top_k is not None:\n",
    "        #     f.write(f\"Top-K: {top_k}\\n\")\n",
    "        # if top_p is not None:\n",
    "        #     f.write(f\"Top-P: {top_p}\\n\")\n",
    "        # f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        # f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        # f.write(\"PROMPT:\\n\")\n",
    "        # f.write(\"=\"*80 + \"\\n\")\n",
    "        # f.write(prompt + \"\\n\")\n",
    "        # f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        # f.write(\"GENERATED STORY:\\n\")\n",
    "        # f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(story_text)\n",
    "        # f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# MAIN GENERATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def generate_all_stories(base_model, tokenizer, output_dir=\"story_output\"):\n",
    "    \"\"\"Generate stories organized by decoding strategy with temperature variations\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING STORIES WITH THE BASE MODEL\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Define all 74 configurations\n",
    "    configurations = [\n",
    "        (\"greedy\", {}),\n",
    "        (\"sampling\", {\"temperature\": 0.1}),\n",
    "        (\"sampling\", {\"temperature\": 0.2}),\n",
    "        (\"sampling\", {\"temperature\": 0.3}),\n",
    "        (\"sampling\", {\"temperature\": 0.4}),\n",
    "        (\"sampling\", {\"temperature\": 0.5}),\n",
    "        (\"sampling\", {\"temperature\": 0.6}),\n",
    "        (\"sampling\", {\"temperature\": 0.7}),\n",
    "        (\"sampling\", {\"temperature\": 0.8}),\n",
    "        (\"sampling\", {\"temperature\": 0.9}),\n",
    "        (\"sampling\", {\"temperature\": 1.0}),\n",
    "        (\"top_k\", {\"temperature\": 0.1, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.2, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.3, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.4, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.5, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.6, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.7, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.8, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.9, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 1.0, \"top_k\": 10}),        \n",
    "        (\"top_k\", {\"temperature\": 0.1, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.2, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.3, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.4, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.5, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.6, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.7, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.8, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.9, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 1.0, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.1, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.2, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.3, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.4, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.5, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.6, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.7, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.8, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.9, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 1.0, \"top_k\": 50}),        \n",
    "        (\"top_p\", {\"temperature\": 0.1, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.2, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.3, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.4, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.5, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.6, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.7, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.8, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.9, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 1.0, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.1, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.2, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.3, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.4, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.5, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.6, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.7, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.8, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.9, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 1.0, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.1, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.2, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.3, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.4, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.5, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.6, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.7, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.8, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.9, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 1.0, \"top_p\": 0.98}),\n",
    "        (\"beam\", {\"num_beams\": 3}),\n",
    "        (\"beam\", {\"num_beams\": 5}),\n",
    "        (\"beam\", {\"num_beams\": 10})\n",
    "    ]\n",
    "    \n",
    "    total_files = len(configurations)\n",
    "    \n",
    "    # Generate stories for each configuration\n",
    "    for idx, (strategy, kwargs) in enumerate(configurations, 1):\n",
    "        # Create descriptive label for filename and display\n",
    "        if strategy == \"greedy\":\n",
    "            label = \"greedy_default\"\n",
    "            display_label = \"Greedy\"\n",
    "        elif strategy == \"sampling\":\n",
    "            temp = kwargs['temperature']\n",
    "            label = f\"sampling_{temp:.1f}\"\n",
    "            display_label = f\"Sampling (T={temp})\"\n",
    "        elif strategy == \"top_k\":\n",
    "            temp = kwargs['temperature']\n",
    "            k = kwargs['top_k']\n",
    "            label = f\"top_k_{temp:.1f}_k{k}\"\n",
    "            display_label = f\"Top-K (T={temp}, K={k})\"\n",
    "        elif strategy == \"top_p\":\n",
    "            temp = kwargs['temperature']\n",
    "            p = kwargs['top_p']\n",
    "            label = f\"top_p_{temp:.1f}_p{p}\"\n",
    "            display_label = f\"Top-P (T={temp}, P={p})\"\n",
    "        elif strategy == \"beam\":\n",
    "            beams = kwargs['num_beams']\n",
    "            label = f\"beam_{beams}\"\n",
    "            display_label = f\"Beam Search (beams={beams})\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{idx}/{total_files}] {display_label}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create strategy folder\n",
    "        strategy_dir = os.path.join(output_dir, strategy)\n",
    "        \n",
    "        # Generate with BASE model\n",
    "        print(f\"Generating with BASE model...\")\n",
    "        \n",
    "        base_story = generate_story(base_model, tokenizer, STORY_PROMPT, strategy=strategy, **kwargs)\n",
    "        \n",
    "        # Create filename\n",
    "        filename = f\"base_{label}.txt\"\n",
    "        filepath = os.path.join(strategy_dir, filename)\n",
    "        \n",
    "        # Save file\n",
    "        save_story_to_file(\n",
    "            filepath, \n",
    "            base_story, \n",
    "            STORY_PROMPT, \n",
    "            strategy, \n",
    "            kwargs.get('temperature'),\n",
    "            \"BASE MODEL\",\n",
    "            top_k=kwargs.get('top_k'),\n",
    "            top_p=kwargs.get('top_p')\n",
    "        )\n",
    "        print(f\"✓ Saved: {filename}\")\n",
    "    \n",
    "    return total_files\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Configuration\n",
    "    BASE_MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "    OUTPUT_DIR = \"story_generation_output\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STORY GENERATION SCRIPT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nBase Model: {BASE_MODEL_ID}\")\n",
    "    print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"\\nStep 1: Loading model...\")\n",
    "    print(\"-\" * 80)\n",
    "    base_model, tokenizer = load_base_model(BASE_MODEL_ID)\n",
    "    \n",
    "    # Generate all stories\n",
    "    print(\"\\nStep 2: Generating stories...\")\n",
    "    print(\"-\" * 80)\n",
    "    total_files = generate_all_stories(base_model, tokenizer, OUTPUT_DIR)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal files generated: {total_files}\")\n",
    "    print(f\"Output location: {OUTPUT_DIR}/\")\n",
    "    print(\"\\nFolder structure:\")\n",
    "    print(f\"  {OUTPUT_DIR}/\")\n",
    "    print(f\"  ├── greedy/\")\n",
    "    print(f\"  │   └── base_greedy_default.txt\")\n",
    "    print(f\"  ├── sampling/\")\n",
    "    print(f\"  │   ├── base_sampling_0.1.txt\")\n",
    "    print(f\"  │   ├── base_sampling_0.2.txt\")\n",
    "    print(f\"  │   └── ... (11 files)\")\n",
    "    print(f\"  ├── top_k/\")\n",
    "    print(f\"  │   └── ... (30 files)\")\n",
    "    print(f\"  ├── top_p/\")\n",
    "    print(f\"  │   └── ... (30 files)\")\n",
    "    print(f\"  └── beam/\")\n",
    "    print(f\"      └── ... (3 files)\")\n",
    "    print(\"\\nEach file contains:\")\n",
    "    print(\"  • Model type\")\n",
    "    print(\"  • Generation parameters\")\n",
    "    print(\"  • The full prompt\")\n",
    "    print(\"  • The generated story text\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
