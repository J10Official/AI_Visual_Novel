{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b92f03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee0887",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e81a5e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# LOAD YOUR FINE-TUNED MODEL\n",
    "# ============================================\n",
    "\n",
    "def load_finetuned_model(lora_path, base_model_id=\"google/gemma-2-2b-it\"):\n",
    "    \"\"\"Load the fine-tuned LoRA model\"\"\"\n",
    "    \n",
    "    print(\"Loading base model with quantization...\")\n",
    "    \n",
    "    # Same quantization config as training\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    # Load LoRA weights\n",
    "    model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"âœ“ Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# GENERATION FUNCTIONS WITH DIFFERENT STRATEGIES\n",
    "# ============================================\n",
    "\n",
    "def generate_story(model, tokenizer, prompt, strategy=\"greedy\", temperature=1.0, top_k=50, top_p=0.9, num_beams=1, max_length=512):\n",
    "    \"\"\"\n",
    "    Generate story with different decoding strategies\n",
    "    \n",
    "    Strategies:\n",
    "    - greedy: Always picks highest probability token (deterministic)\n",
    "    - sampling: Random sampling with temperature control\n",
    "    - top_k: Sample from top K most likely tokens\n",
    "    - top_p: Nucleus sampling - sample from smallest set of tokens with cumulative prob >= p\n",
    "    - beam: Beam search (explores multiple paths)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format input in Gemma's chat template\n",
    "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generation parameters based on strategy\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": max_length,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    if strategy == \"greedy\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": False,\n",
    "            \"num_beams\": 1,\n",
    "        })\n",
    "        \n",
    "    elif strategy == \"sampling\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "        })\n",
    "        \n",
    "    elif strategy == \"top_k\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": top_k,\n",
    "        })\n",
    "        \n",
    "    elif strategy == \"top_p\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "        })\n",
    "        \n",
    "    elif strategy == \"beam\":\n",
    "        gen_kwargs.update({\n",
    "            \"do_sample\": False,\n",
    "            \"num_beams\": num_beams,\n",
    "            \"early_stopping\": True,\n",
    "        })\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the model's response\n",
    "    if \"<start_of_turn>model\" in generated_text:\n",
    "        response = generated_text.split(\"<start_of_turn>model\")[-1].strip()\n",
    "    else:\n",
    "        response = generated_text\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION WITH ALL CONFIGURATIONS\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Load your fine-tuned model\n",
    "    LORA_PATH = \"/kaggle/input/gemma-2-gold/transformers/default/1\"  # Path to your saved LoRA adapter\n",
    "    \n",
    "    model, tokenizer = load_finetuned_model(LORA_PATH)\n",
    "    \n",
    "    # Story prompt with woodcutter and pond\n",
    "    prompt = \"Create a scene at a mystical pond with mood: mysterious. Characters: Woodcutter, Pond Spirit\"\n",
    "    \n",
    "    # Define all 74 configurations\n",
    "    configurations = [\n",
    "        (\"greedy\", {}),\n",
    "        (\"sampling\", {\"temperature\": 0.1}),\n",
    "        (\"sampling\", {\"temperature\": 0.2}),\n",
    "        (\"sampling\", {\"temperature\": 0.3}),\n",
    "        (\"sampling\", {\"temperature\": 0.4}),\n",
    "        (\"sampling\", {\"temperature\": 0.5}),\n",
    "        (\"sampling\", {\"temperature\": 0.6}),\n",
    "        (\"sampling\", {\"temperature\": 0.7}),\n",
    "        (\"sampling\", {\"temperature\": 0.8}),\n",
    "        (\"sampling\", {\"temperature\": 0.9}),\n",
    "        (\"sampling\", {\"temperature\": 1.0}),\n",
    "        (\"top_k\", {\"temperature\": 0.1, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.2, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.3, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.4, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.5, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.6, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.7, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.8, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 0.9, \"top_k\": 10}),\n",
    "        (\"top_k\", {\"temperature\": 1.0, \"top_k\": 10}),        \n",
    "        (\"top_k\", {\"temperature\": 0.1, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.2, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.3, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.4, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.5, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.6, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.7, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.8, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.9, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 1.0, \"top_k\": 30}),\n",
    "        (\"top_k\", {\"temperature\": 0.1, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.2, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.3, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.4, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.5, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.6, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.7, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.8, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 0.9, \"top_k\": 50}),\n",
    "        (\"top_k\", {\"temperature\": 1.0, \"top_k\": 50}),        \n",
    "        (\"top_p\", {\"temperature\": 0.1, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.2, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.3, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.4, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.5, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.6, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.7, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.8, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.9, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 1.0, \"top_p\": 0.5}),\n",
    "        (\"top_p\", {\"temperature\": 0.1, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.2, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.3, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.4, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.5, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.6, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.7, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.8, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.9, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 1.0, \"top_p\": 0.9}),\n",
    "        (\"top_p\", {\"temperature\": 0.1, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.2, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.3, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.4, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.5, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.6, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.7, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.8, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 0.9, \"top_p\": 0.98}),\n",
    "        (\"top_p\", {\"temperature\": 1.0, \"top_p\": 0.98}),\n",
    "        (\"beam\", {\"num_beams\": 3}),\n",
    "        (\"beam\", {\"num_beams\": 5}),\n",
    "        (\"beam\", {\"num_beams\": 10})\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"GENERATING STORIES WITH {len(configurations)} CONFIGURATIONS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Store all results\n",
    "    results = []\n",
    "    \n",
    "    # Generate stories for all configurations\n",
    "    for idx, (strategy, kwargs) in enumerate(configurations, 1):\n",
    "        # Create descriptive label\n",
    "        if strategy == \"greedy\":\n",
    "            label = \"Greedy\"\n",
    "        elif strategy == \"sampling\":\n",
    "            label = f\"Sampling (T={kwargs['temperature']})\"\n",
    "        elif strategy == \"top_k\":\n",
    "            label = f\"Top-K (T={kwargs['temperature']}, K={kwargs['top_k']})\"\n",
    "        elif strategy == \"top_p\":\n",
    "            label = f\"Top-P (T={kwargs['temperature']}, P={kwargs['top_p']})\"\n",
    "        elif strategy == \"beam\":\n",
    "            label = f\"Beam Search (beams={kwargs['num_beams']})\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Configuration {idx}/{len(configurations)}: {label}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        try:\n",
    "            story = generate_story(model, tokenizer, prompt, strategy=strategy, **kwargs)\n",
    "            print(story)\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                \"config_id\": idx,\n",
    "                \"strategy\": strategy,\n",
    "                \"parameters\": kwargs,\n",
    "                \"label\": label,\n",
    "                \"story\": story\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            results.append({\n",
    "                \"config_id\": idx,\n",
    "                \"strategy\": strategy,\n",
    "                \"parameters\": kwargs,\n",
    "                \"label\": label,\n",
    "                \"story\": f\"ERROR: {e}\"\n",
    "            })\n",
    "    \n",
    "    # Save results to JSON file\n",
    "    output_file = f\"story_generations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"prompt\": prompt,\n",
    "            \"total_configurations\": len(configurations),\n",
    "            \"results\": results\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"GENERATION COMPLETE!\")\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    print(f\"Total configurations tested: {len(configurations)}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ANALYSIS FUNCTION (OPTIONAL)\n",
    "# ============================================\n",
    "\n",
    "def analyze_results(results_file):\n",
    "    \"\"\"Analyze the generated stories to find patterns\"\"\"\n",
    "    \n",
    "    with open(results_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = data['results']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS OF GENERATED STORIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by strategy\n",
    "    strategy_groups = {}\n",
    "    for r in results:\n",
    "        strat = r['strategy']\n",
    "        if strat not in strategy_groups:\n",
    "            strategy_groups[strat] = []\n",
    "        strategy_groups[strat].append(r)\n",
    "    \n",
    "    print(f\"\\nTotal stories generated: {len(results)}\")\n",
    "    print(\"\\nBreakdown by strategy:\")\n",
    "    for strat, stories in strategy_groups.items():\n",
    "        print(f\"  {strat}: {len(stories)} stories\")\n",
    "    \n",
    "    # Find unique stories\n",
    "    unique_stories = {}\n",
    "    for r in results:\n",
    "        story = r['story']\n",
    "        if story not in unique_stories:\n",
    "            unique_stories[story] = []\n",
    "        unique_stories[story].append(r['label'])\n",
    "    \n",
    "    print(f\"\\nUnique stories generated: {len(unique_stories)}\")\n",
    "    print(f\"Duplicate stories: {len(results) - len(unique_stories)}\")\n",
    "    \n",
    "    # Show most common stories\n",
    "    story_counts = [(story, len(configs)) for story, configs in unique_stories.items()]\n",
    "    story_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n--- Most common outputs (duplicates) ---\")\n",
    "    for story, count in story_counts[:5]:\n",
    "        if count > 1:\n",
    "            print(f\"\\nAppeared {count} times:\")\n",
    "            print(f\"Configs: {', '.join(unique_stories[story])}\")\n",
    "            print(f\"Story preview: {story[:150]}...\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# INTERACTIVE MODE (OPTIONAL)\n",
    "# ============================================\n",
    "\n",
    "def interactive_generation(model, tokenizer):\n",
    "    \"\"\"Interactive story generation with all configurations available\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERACTIVE STORY GENERATOR\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Build menu from configurations\n",
    "    configurations = [\n",
    "        (\"greedy\", {}),\n",
    "        (\"sampling\", {\"temperature\": 0.1}),\n",
    "        (\"sampling\", {\"temperature\": 0.5}),\n",
    "        (\"sampling\", {\"temperature\": 0.7}),\n",
    "        (\"sampling\", {\"temperature\": 1.0}),\n",
    "        (\"top_k\", {\"temperature\": 0.8, \"top_k\": 30}),\n",
    "        (\"top_p\", {\"temperature\": 0.8, \"top_p\": 0.9}),\n",
    "        (\"beam\", {\"num_beams\": 5}),\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        prompt = input(\"\\nEnter your prompt (or 'quit' to exit): \")\n",
    "        if prompt.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        print(\"\\nSelect decoding strategy:\")\n",
    "        for i, (strat, kwargs) in enumerate(configurations, 1):\n",
    "            if strat == \"greedy\":\n",
    "                print(f\"{i}. Greedy\")\n",
    "            elif strat == \"sampling\":\n",
    "                print(f\"{i}. Sampling (T={kwargs['temperature']})\")\n",
    "            elif strat == \"top_k\":\n",
    "                print(f\"{i}. Top-K (T={kwargs['temperature']}, K={kwargs['top_k']})\")\n",
    "            elif strat == \"top_p\":\n",
    "                print(f\"{i}. Top-P (T={kwargs['temperature']}, P={kwargs['top_p']})\")\n",
    "            elif strat == \"beam\":\n",
    "                print(f\"{i}. Beam Search (beams={kwargs['num_beams']})\")\n",
    "        \n",
    "        choice = input(f\"\\nChoice (1-{len(configurations)}): \")\n",
    "        \n",
    "        try:\n",
    "            idx = int(choice) - 1\n",
    "            if 0 <= idx < len(configurations):\n",
    "                strategy, kwargs = configurations[idx]\n",
    "                print(f\"\\nGenerating with {strategy}...\")\n",
    "                story = generate_story(model, tokenizer, prompt, strategy=strategy, **kwargs)\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"GENERATED STORY:\")\n",
    "                print(\"=\"*80)\n",
    "                print(story)\n",
    "            else:\n",
    "                print(\"Invalid choice!\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input!\")\n",
    "\n",
    "# Uncomment to run interactive mode\n",
    "# interactive_generation(model, tokenizer)\n",
    "\n",
    "# Uncomment to analyze results from a previous run\n",
    "# analyze_results(\"story_generations_20241106_123456.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
